{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCombining COPT with JAX\n=======================\n\nThis example shows how JAX can be used within COPT\nto compute the gradients of the objective function.\nIn this example tensorflow-datasets is used to provide\nthe training data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copt as cp\nimport jax.numpy as np\nfrom jax import random\nfrom jax import vmap\nfrom jax import grad\n# from jax.experimental import stax\n# from jax.experimental.stax import Dense, Relu, LogSoftmax\nimport tensorflow_datasets as tfds\n\n\ndef one_hot(x, k, dtype=np.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return np.array(x[:, None] == np.arange(k), dtype)\n\n# as_supervised=True gives us the (image, label) as a tuple instead of a dict\nds, info = tfds.load(name='mnist', split='train', as_supervised=True, with_info=True)\n# You can build up an arbitrary tf.data input pipeline\nds = ds.batch(128).prefetch(1)\n# tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\nbatches = tfds.as_numpy(ds)\nnum_labels = info.features['label'].num_classes\nh, w, c = info.features['image'].shape\nnum_pixels = h * w * c\n\n\nfrom jax.scipy.special import logsumexp\n\ndef relu(x):\n  return np.maximum(0, x)\n\ndef predict(params, image):\n  # per-example predictions\n  activations = image\n  for w, b in params[:-1]:\n    outputs = np.dot(w, activations) + b\n    activations = relu(outputs)\n\n  final_w, final_b = params[-1]\n  logits = np.dot(final_w, activations) + final_b\n  return logits - logsumexp(logits)\n\ndef loss(params, images, targets):\n  batched_predict = vmap(predict, in_axes=(None, 0))\n  preds = batched_predict(params, images)\n  return -np.sum(preds * targets)\n\ndef f_grad(params):\n  x, y = next(batches)\n  x = np.reshape(x, (len(x), num_pixels))\n  y = one_hot(y, num_labels)\n  func = loss(params, x, y)\n  func_grad = grad(loss)(params, x, y)\n  return func, func_grad\n\n\ndef random_layer_params(m, n, key, scale=1e-2):\n  w_key, b_key = random.split(key)\n  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n\n\n# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\ndef init_network_params(sizes, key):\n  keys = random.split(key, len(sizes))\n  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n\nlayer_sizes = [784, 512, 512, 10]\nparams = init_network_params(layer_sizes, random.PRNGKey(0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}