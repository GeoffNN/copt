{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGroup lasso with overlap\n========================\n\nComparison of solvers for a least squares with\noverlapping group lasso regularization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom scipy.sparse import linalg as splinalg\nfrom sklearn import datasets\nimport pylab as plt\nimport copt as cp\n\nnp.random.seed(0)\n\n# .. generate some data ..\n\nn_samples, n_features = 100, 100\ngroups = [np.arange(8 * i, 8 * i + 10) for i in range(n_features // 8)]\n\n\nground_truth = np.zeros(n_features)\nground_truth[groups[4]] = 1\nground_truth[groups[5]] = 0.5\n\n\nA = np.random.randn(n_samples, n_features)\np = 0.5\nfor i in range(1, n_features):\n    A[:, i] = p * A[:, i] + (1 - p) * A[:, i-1]\nA[:, 0] /= np.sqrt(1 - p ** 2)\nsigma = 1.\nb = A.dot(ground_truth) + sigma * np.random.randn(n_samples)\n\n\n# .. compute the step-size ..\nmax_iter = 5000\ns = splinalg.svds(A, k=1, return_singular_vectors=False)[0]\nstep_size = 1. / cp.utils.get_lipschitz(A, 'square')\nprint(step_size)\nf = cp.utils.SquareLoss(A, b)\n\n# .. run the solver for different values ..\n# .. of the regularization parameter beta ..\nall_betas = [0, 1e-3, 1e-2, 1e-1]\nall_trace_ls, all_trace_nols, all_trace_pdhg_nols, all_trace_pdhg = [], [], [], []\nall_trace_ls_time, all_trace_nols_time, all_trace_pdhg_nols_time, all_trace_pdhg_time = [], [], [], []\nout_img = []\nfor i, beta in enumerate(all_betas):\n    print('beta = %s' % beta)\n    groups1, groups2 = groups[::2], groups[1::2].copy()\n    G1 = cp.utils.GroupL1(beta, groups1)\n    G2 = cp.utils.GroupL1(beta, groups2)\n\n    def loss(x):\n        return f(x) + G1(x) + G2(x)\n\n    cb_tosls = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cb_tosls(x0)\n    tos_ls = cp.minimize_TOS(\n        f.func_grad, x0, G1.prox, G2.prox, step_size=3 * step_size,\n        max_iter=max_iter, tol=1e-14, verbose=1,\n        callback=cb_tosls, h_Lipschitz=beta)\n    trace_ls = np.array([loss(x) for x in cb_tosls.trace_x])\n    all_trace_ls.append(trace_ls)\n    all_trace_ls_time.append(cb_tosls.trace_time)\n\n    cb_tos = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cb_tos(x0)\n    tos = cp.minimize_TOS(\n        f.func_grad, x0, G1.prox, G2.prox,\n        step_size=step_size,\n        max_iter=max_iter, tol=1e-14, verbose=1,\n        line_search=True, callback=cb_tos)\n    trace_nols = np.array([loss(x) for x in cb_tos.trace_x])\n    all_trace_nols.append(trace_nols)\n    all_trace_nols_time.append(cb_tos.trace_time)\n    out_img.append(tos.x)\n\n    cb_pdhg = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cb_pdhg(x0)\n    pdhg = cp.gradient.minimize_PDHG(\n        f.func_grad, x0, G1.prox, G2.prox,\n        callback=cb_pdhg, max_iter=max_iter,\n        step_size=step_size,\n        step_size2=(1. / step_size) / 2, tol=0, line_search=False)\n    trace_pdhg = np.array([loss(x) for x in cb_pdhg.trace_x])\n    all_trace_pdhg.append(trace_pdhg)\n    all_trace_pdhg_time.append(cb_pdhg.trace_time)\n\n    cb_pdhg_nols = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cb_pdhg_nols(x0)\n    pdhg_nols = cp.gradient.minimize_PDHG(\n        f.func_grad, x0, G1.prox, G2.prox,\n        callback=cb_pdhg_nols, max_iter=max_iter,\n        step_size=step_size,\n        step_size2=(1. / step_size) / 2, tol=0, line_search=False)\n    trace_pdhg_nols = np.array([loss(x) for x in cb_pdhg_nols.trace_x])\n    all_trace_pdhg_nols.append(trace_pdhg_nols)\n    all_trace_pdhg_nols_time.append(cb_pdhg_nols.trace_time)\n    #\n\n# .. plot the results ..\nfig, ax = plt.subplots(2, 4, sharey=False)\nxlim = [0.02, 0.02, 0.1]\nfor i, beta in enumerate(all_betas):\n    ax[0, i].set_title(r'$\\lambda=%s$' % beta)\n    ax[0, i].set_title(r'$\\lambda=%s$' % beta)\n    ax[0, i].plot(out_img[i])\n    ax[0, i].plot(ground_truth)\n    ax[0, i].set_xticks(())\n    ax[0, i].set_yticks(())\n\n    fmin = min(np.min(all_trace_ls[i]), np.min(all_trace_nols[i]))\n    scale = 1. # all_trace_ls[i][0] - fmin\n    plot_tos, = ax[1, i].plot(\n        (all_trace_ls[i] - fmin) / scale,\n        lw=4, marker='o', markevery=100,\n        markersize=10)\n\n    plot_nols, = ax[1, i].plot(\n        (all_trace_nols[i] - fmin) / scale,\n        lw=4, marker='h', markevery=100,\n        markersize=10)\n\n    plot_pdhg, = ax[1, i].plot(\n        (all_trace_pdhg[i] - fmin) / scale,\n        lw=4, marker='^', markevery=100,\n        markersize=10)\n\n    plot_pdhg_nols, = ax[1, i].plot(\n        (all_trace_pdhg_nols[i] - fmin) / scale,\n        lw=4, marker='d', markevery=100,\n        markersize=10)\n\n    ax[1, i].set_xlabel('Iterations')\n    ax[1, i].set_yscale('log')\n    ax[1, i].set_ylim((1e-14, None))\n    ax[1, i].grid(True)\n\n\nplt.gcf().subplots_adjust(bottom=0.15)\nplt.figlegend(\n    (plot_tos, plot_nols, plot_pdhg, plot_pdhg_nols),\n    ('TOS with line search', 'TOS without line search', 'PDHG LS', 'PDHG no LS'), ncol=5,\n    scatterpoints=1,\n    loc=(-0.00, -0.0), frameon=False,\n    bbox_to_anchor=[0.05, 0.01])\n\nax[1, 0].set_ylabel('Objective minus optimum')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}