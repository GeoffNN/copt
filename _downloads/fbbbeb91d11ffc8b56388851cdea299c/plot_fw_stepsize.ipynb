{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nStep-size and curvature on the Frank-Wolfe algorithm\n=====================================================\n\nPlot showing both the optimal step-size and norm of Hessian\nfor the Frank-Wolfe algorithm on a logistic regression problem.\n\nThe step-size is computed as the one that gives the largest\ndecrease in objective function (see :func:`exact_ls`). The\nnorm of the Hessian is its largest singular value.\n\nIn the plot we can see how the variance of the step-size\nis much higher than the one associated with the Hessian's\nnorm.\n\nReferences\n----------\nhttp://fa.bianp.net/blog/2019/adaptive_fw/\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\nfrom sklearn import datasets\nfrom scipy import optimize\nfrom scipy.sparse import linalg as splinalg\nimport numpy as np\nimport copt as cp\n\n# Construct a toy classification dataset with 100 samples and 10 features\nn_samples, n_features = 100, 10\nX, y = datasets.make_classification(n_samples, n_features, random_state=0)\n\n\n# Define an exact line search strategy\ndef exact_ls(kw):\n    def f_ls(gamma):\n        return kw['f_grad'](kw['x'] + gamma * kw['d_t'])[0]\n    ls_sol = optimize.minimize_scalar(f_ls, bounds=[0, 1], method='bounded')\n    return ls_sol.x\n\n\nl1_ball = cp.utils.L1Ball(n_features / 2.)\nf = cp.utils.LogLoss(X, y)\nx0 = np.zeros(n_features)\ntrace_step_size = []\ntrace_hessian_norm = []\n\n\ndef cb(kw):\n    trace_step_size.append(kw['step_size'])\n    Hs = splinalg.LinearOperator(\n        shape=(n_features, n_features),\n        matvec=f.Hessian(kw['x']))\n\n    s, _ = splinalg.eigsh(Hs, k=1)\n    trace_hessian_norm.append(s)\n\n\nout = cp.minimize_FW(\n    f.f_grad, l1_ball.lmo, x0, callback=cb, max_iter=1000,\n    backtracking=exact_ls)\n\n# Focus on the last 4/5, since the first iterations\n# tend to have a disproportionally large step-size\nn = len(trace_step_size) // 5\ntrace_step_size = trace_step_size[n:]\ntrace_hessian_norm = trace_hessian_norm[n:]\n\n\nfig, ax1 = plt.subplots()\n\ncolor = '#67a9cf'\nax1.set_xlabel('number of iterations')\nax1.set_ylabel('step-size', color=color)\nax1.plot(n + np.arange(len(trace_step_size)), trace_step_size, color=color,\n    alpha=0.5)\nax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = '#ef8a62'\nax2.set_ylabel('Hessian norm', color=color)  # we already handled the x-label with ax1\nax2.plot(n + np.arange(len(trace_hessian_norm)), trace_hessian_norm, color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.xlim(n, n + len(trace_step_size))\nplt.grid()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}