.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_jax_copt.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_jax_copt.py:


Combining COPT with JAX
=======================

This example shows how JAX can be used within COPT
to compute the gradients of the objective function.
In this example tensorflow-datasets is used to provide
the training data.


.. code-block:: default

    import copt as cp
    import jax.numpy as np
    import numpy as onp
    from jax import random
    from jax import vmap
    from jax import grad
    import pylab as plt

    # .. construct (random) dataset ..
    n_samples, n_features = 1000, 200
    key = random.PRNGKey(1)
    X = random.normal(key, (n_samples, n_features))
    key, subkey = random.split(key)
    y = random.normal(key, (n_samples,))


    def loss(w):
      # squared error loss
      z = X.dot(w) - y
      return np.sum(z * z)

    def f_grad(w):
      return loss(w), grad(loss)(w)

    w0 = onp.zeros(n_features)

    l1_ball = cp.utils.L1Ball(n_features / 2.)
    cb = cp.utils.Trace(loss)
    cp.minimize_proximal_gradient(
        f_grad,
        w0,
        verbose=True,
        callback=cb
    )
    plt.plot(cb.trace_fx)
    plt.xlabel('# Iterations')
    plt.ylabel('Objective value')
    plt.grid()
    plt.show()

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_examples_jax_copt.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: jax_copt.py <jax_copt.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: jax_copt.ipynb <jax_copt.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
